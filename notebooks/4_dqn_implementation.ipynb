{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553fd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51918617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le chemin du dossier parent pour importer environment_setup.py\n",
    "sys.path.append('..')\n",
    "from environment_setup import ScheduleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8ada6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la version de TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b5919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins de fichiers\n",
    "DATA_PATH = \"../Data/processed/preprocessed_data.csv\"\n",
    "MODEL_PATH = \"../models/dqn_model\"\n",
    "os.makedirs(\"../models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6445436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données prétraitées...\n",
      "Données chargées: 37933 entrées\n"
     ]
    }
   ],
   "source": [
    "# 1. Charger les données prétraitées\n",
    "print(\"Chargement des données prétraitées...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Données chargées: {df.shape[0]} entrées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d1b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environnement créé avec 18 actions possibles\n"
     ]
    }
   ],
   "source": [
    "# Configurer l'environnement\n",
    "env = ScheduleEnv(df)\n",
    "print(f\"Environnement créé avec {env.action_space.n} actions possibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b1ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implémentation de la mémoire d'expérience (Experience Replay)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acfcf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Implémentation de l'agent DQN\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Paramètres de l'algorithme\n",
    "        self.gamma = 0.95  # Facteur d'actualisation\n",
    "        self.epsilon = 1.0  # Exploration initiale\n",
    "        self.epsilon_min = 0.01  # Exploration minimale\n",
    "        self.epsilon_decay = 0.995  # Taux de décroissance de l'exploration\n",
    "        self.learning_rate = 0.001  # Taux d'apprentissage\n",
    "        self.batch_size = 64  # Taille des batchs d'apprentissage\n",
    "        \n",
    "        # Mémoire d'expérience\n",
    "        self.memory = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "        # Réseaux de neurones (principal et cible)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Réseau de neurones pour approximer la fonction Q\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(self.state_size,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # Copier les poids du modèle principal vers le modèle cible\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Stocker l'expérience dans la mémoire\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        # Stratégie epsilon-greedy pour l'exploration/exploitation\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Prédire les valeurs Q pour toutes les actions\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        # Apprentissage par expérience replay\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Échantillonner un batch de la mémoire\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Calculer les valeurs Q cibles\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        # Mettre à jour epsilon pour réduire l'exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5755c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fonction d'entraînement\n",
    "def train_dqn(env, agent, episodes=1000, update_target_every=10, max_steps=24):\n",
    "    \"\"\"\n",
    "    Entraîne l'agent DQN sur l'environnement spécifié\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Réinitialiser l'environnement\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choisir une action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Exécuter l'action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Stocker l'expérience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Mettre à jour l'état\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Apprentissage\n",
    "            agent.replay()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Mettre à jour le modèle cible périodiquement\n",
    "        if episode % update_target_every == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # Afficher la progression\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}/{episodes}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "965df8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualisation des résultats\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Récompenses par épisode')\n",
    "    plt.xlabel('Épisode')\n",
    "    plt.ylabel('Récompense totale')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('../outputs/dqn_rewards.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fb448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Évaluation du modèle\n",
    "def evaluate_agent(env, agent, episodes=10):\n",
    "    \"\"\"\n",
    "    Évalue les performances de l'agent entraîné\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Évaluation du modèle ---\")\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state, training=False)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Épisode {episode+1}: Récompense = {total_reward:.2f}\")\n",
    "    \n",
    "    print(f\"\\nRécompense moyenne: {np.mean(total_rewards):.2f}\")\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État: 27 dimensions, Actions: 18 possibilités\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Début de l'entraînement ---\n",
      "Episode: 0/500, Reward: 13.50, Epsilon: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 7. Exécution de l'entraînement et de l'évaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    print(f\"État: {state_size} dimensions, Actions: {action_size} possibilités\")\n",
    "    \n",
    "    # Créer l'agent\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    # Entraîner l'agent\n",
    "    print(\"\\n--- Début de l'entraînement ---\")\n",
    "    rewards = train_dqn(env, agent, episodes=500, update_target_every=5)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    agent.save(MODEL_PATH)\n",
    "    print(f\"\\nModèle sauvegardé dans {MODEL_PATH}\")\n",
    "    \n",
    "    # Visualiser les résultats\n",
    "    plot_rewards(rewards)\n",
    "    \n",
    "    # Évaluer l'agent\n",
    "    evaluate_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Visualisation des plannings générés\n",
    "def visualize_schedule(env, agent):\n",
    "    \"\"\"\n",
    "    Génère et visualise un planning optimisé par l'agent\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    activities = []\n",
    "    slots = []\n",
    "    \n",
    "    for slot in range(24):\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Récupérer le nom de l'activité\n",
    "        activity_name = df[df['ACTIVITY_NAME_ENC'] == action]['ACTIVITY_NAME'].mode()\n",
    "        if len(activity_name) > 0:\n",
    "            activity = activity_name.iloc[0]\n",
    "        else:\n",
    "            activity = f\"Activity {action}\"\n",
    "        \n",
    "        activities.append(activity)\n",
    "        slots.append(slot)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Créer un DataFrame pour la visualisation\n",
    "    schedule_df = pd.DataFrame({\n",
    "        'Heure': slots,\n",
    "        'Activité': activities\n",
    "    })\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(schedule_df['Heure'], [1] * len(schedule_df), color='skyblue')\n",
    "    \n",
    "    # Ajouter les noms des activités\n",
    "    for i, (hour, activity) in enumerate(zip(schedule_df['Heure'], schedule_df['Activité'])):\n",
    "        plt.text(0.5, hour, activity, ha='center', va='center')\n",
    "    \n",
    "    plt.yticks(slots, [f\"{h}:00\" for h in slots])\n",
    "    plt.xlabel('Activité')\n",
    "    plt.ylabel('Heure')\n",
    "    plt.title('Planning journalier optimisé')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.savefig('../outputs/optimized_schedule.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dossier de sortie si nécessaire\n",
    "os.makedirs(\"../outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer un exemple de planning\n",
    "print(\"\\n--- Génération d'un planning optimisé ---\")\n",
    "visualize_schedule(env, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les informations sur la taille de la mémoire d'expérience\n",
    "print(f\"\\nTaille de la mémoire d'expérience: {agent.memory.size()} échantillons\")\n",
    "print(f\"Taux d'exploration final: {agent.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b07b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
